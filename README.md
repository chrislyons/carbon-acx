# Carbon ACX

Carbon ACX is a reference implementation for building reproducible carbon accounting datasets and experiences. It demonstrates how to ingest heterogeneous activity data, normalise it into shared emissions models, derive decision-ready artefacts, and deliver those artefacts across interactive and static channels.

---

## Table of contents

1. [Project overview](#project-overview)
   - [Mission & scope](#mission--scope)
   - [Architecture & problem domain](#architecture--problem-domain)
   - [Positioning](#positioning)
   - [Layer catalogue at-a-glance](#layer-catalogue-at-a-glance)
2. [Features](#features)
3. [Repository structure](#repository-structure)
4. [Installation & setup](#installation--setup)
   - [Local development](#local-development)
   - [Database-backed workflows](#database-backed-workflows)
   - [Front-end toolchain](#front-end-toolchain)
   - [Production preparation](#production-preparation)
5. [Usage](#usage)
   - [Derivation CLI](#derivation-cli)
   - [Intensity matrix CLI](#intensity-matrix-cli)
   - [Dash exploration client](#dash-exploration-client)
   - [Static site bundle](#static-site-bundle)
   - [Programmatic aggregates](#programmatic-aggregates)
   - [On-demand compute service](#on-demand-compute-service)
   - [Diagnostics & utilities](#diagnostics--utilities)
6. [Configuration](#configuration)
   - [Environment variables](#environment-variables)
   - [Configuration files & flags](#configuration-files--flags)
   - [Preset data](#preset-data)
7. [Build & deployment](#build--deployment)
8. [Testing & QA](#testing--qa)
9. [Contributing guide](#contributing-guide)
10. [Security & compliance](#security--compliance)
11. [Roadmap](#roadmap)
12. [FAQ & troubleshooting](#faq--troubleshooting)
13. [References](#references)

---

## Project overview

### Mission & scope

Carbon ACX exists to make carbon accounting datasets portable. The project:

- Converts raw activity records into well-defined emissions models using Pydantic validation in [`calc/schema.py`](./calc/schema.py), ensuring unit registries, layer enumerations, and grid strategies remain consistent.ã€F:calc/schema.pyâ€ L1-L101ã€‘
- Computes annualised emissions, intensity matrices, uncertainty bounds, and manifest metadata to create a portable bundle of JSON, CSV, and reference text files via [`calc/derive.py`](./calc/derive.py).ã€F:calc/derive.pyâ€ L1-L114ã€‘
- Ships both a Dash-based exploration environment and a static React/Vite experience that render directly from the derived bundle, proving that the data products are channel-agnostic.ã€F:app/app.pyâ€ L1-L132ã€‘ã€F:site/src/App.tsxâ€ L1-L58ã€‘
- Provides an API-friendly surface (`calc.api`/`calc.service`) plus a Cloudflare Worker runtime so downstream services can request fresh figures without duplicating the derivation logic.ã€F:calc/service.pyâ€ L1-L123ã€‘ã€F:workers/compute/index.tsâ€ L1-L86ã€‘

### Architecture & problem domain

The repository is organised around a **derive-once, serve-anywhere** architecture:

1. **Domain modelling (`calc/schema.py`)** â€” Canonical CSV inputs are typed with Pydantic models enforcing unit registries (`units.csv`), scope boundaries, grid intensity schemas, and functional unit conversion formula parsing. A read-through cache keeps repeated loads deterministic.ã€F:calc/schema.pyâ€ L1-L87ã€‘ã€F:calc/derive.pyâ€ L160-L240ã€‘
2. **Data access (`calc/dal/`)** â€” A pluggable `DataStore` abstraction supports CSV (`CsvStore`), DuckDB (`DuckDbStore`), and SQLite/DuckDB SQL stores (`SqlStore`) so derivations can run against files, in-memory databases, or production-ready connections.ã€F:calc/dal/__init__.pyâ€ L1-L66ã€‘ã€F:calc/dal_sql.pyâ€ L25-L115ã€‘
3. **Computation (`calc/derive.py`)** â€” Activity schedules, emission factors, and grid intensities are combined into annualised emissions with guardrails for layer attribution, reference tracking, hashing, manifest metadata, and safe output directory handling.ã€F:calc/derive.pyâ€ L292-L382ã€‘ã€F:calc/derive.pyâ€ L1500-L1599ã€‘
4. **Presentation (`calc/figures.py`, `app/components`, `site/`)** â€” Plotly figure slices, disclosure copy, IEEE references, and UI theming are produced once and consumed by both Dash components and the static React client so user experiences stay consistent across channels.ã€F:calc/figures.pyâ€ L1-L120ã€‘ã€F:app/components/_plotly_settings.pyâ€ L1-L40ã€‘ã€F:site/src/components/VizCanvas.tsxâ€ L1-L120ã€‘
5. **Delivery (`scripts/*.py`, `functions/carbon-acx`, `workers/compute`)** â€” Automation packages the derived bundle, renders the static client, publishes Cloudflare Pages metadata, and proxies production traffic while enforcing caching, routing, and optional upstream fallbacks.ã€F:scripts/prepare_pages_bundle.pyâ€ L1-L82ã€‘ã€F:functions/carbon-acx/[[path]].tsâ€ L1-L200ã€‘

This architecture targets carbon accounting teams that need a transparent pipeline from data ingestion through public presentation.

### Positioning

Compared to ad-hoc spreadsheets or monolithic BI stacks, Carbon ACX emphasises:

- **Deterministic builds** â€” Artefacts are versioned by content hash and generated through a single CLI entry point that only clears whitelisted directories (`calc/derive.is_safe_output_dir`).ã€F:calc/derive.pyâ€ L252-L336ã€‘
- **Reference integrity** â€” Every derived figure carries IEEE-formatted citations sourced from curated reference files (`calc/references/*.txt`) via the `calc.citations` resolver.ã€F:calc/citations.pyâ€ L1-L74ã€‘ã€F:calc/derive.pyâ€ L1474-L1526ã€‘
- **Channel parity** â€” The same derived payload drives Dash, the static React/Vite site, and the Cloudflare Worker API, simplifying validation and deployment while keeping disclosure copy and NA notices aligned.ã€F:app/app.pyâ€ L70-L129ã€‘ã€F:site/src/routes/Story.tsxâ€ L1-L160ã€‘
- **Backend agility** â€” `ACX_DATA_BACKEND` toggles between CSV, DuckDB, and SQLite stores; `scripts/import_csv_to_db.py` and `scripts/export_db_to_csv.py` keep SQL backends in sync with the canonical CSVs.ã€F:calc/dal/__init__.pyâ€ L35-L78ã€‘ã€F:scripts/import_csv_to_db.pyâ€ L1-L120ã€‘
- **Test-first data hygiene** â€” A broad pytest suite covers schema constraints, backend parity, manifest metadata, API responses, and UI snapshots; the static front-end ships with Vitest checks for React behaviour.ã€F:tests/test_schema.pyâ€ L1-L200ã€‘ã€F:tests/test_backend_parity.pyâ€ L1-L47ã€‘ã€F:site/package.jsonâ€ L1-L34ã€‘

### Layer catalogue at-a-glance

| Layer | Summary | Example activities |
| --- | --- | --- |
| Professional services | Baseline knowledge worker footprint anchored to hybrid office routines. | Coffeeâ€”12 oz hot Â· Toronto subwayâ€”per passenger-kilometre |
| Online services | SaaS, meetings, and streaming workloads for remote-first teams. | Video conferencing hour Â· SaaS productivity suite seat |
| Industrial (Light) | Lab, prototyping, and light fabrication scenarios for innovation hubs. | Lab bench operation Â· Prototyping print run |
| Industrial (Heavy) | Full-scale manufacturing and heavy industry references for R&D insight. | Steel batch furnace Â· Heavy equipment runtime |

The layer catalogue is sourced from [`data/layers.csv`](./data/layers.csv) and mirrored for the site bundle in [`site/public/artifacts/layers.json`](./site/public/artifacts/layers.json). Run `python scripts/audit_layers.py` to regenerate the discovery report (`artifacts/audit_report.json`), which lists every seeded activity, operation coverage, icon status, and UI wiring health for quick QA.ã€F:data/layers.csvâ€ L1-L40ã€‘ã€F:scripts/audit_layers.pyâ€ L1-L120ã€‘

---

## Features

- **Typed data ingestion** â€” `calc/schema` loads CSV inputs with unit registry validation, functional unit formula evaluation, and cached reads for repeat derivations.ã€F:calc/schema.pyâ€ L1-L167ã€‘ã€F:calc/derive.pyâ€ L140-L212ã€‘
- **Backend-agnostic datastore** â€” `calc.dal.choose_backend` selects CSV, DuckDB, or SQLite implementations; `ACX_DB_PATH` allows pointing SQL stores at alternate paths.ã€F:calc/dal/__init__.pyâ€ L55-L90ã€‘
- **Emission computation engine** â€” `calc.derive.export_view` applies profile schedules, grid intensities, uncertainty bounds, upstream dependency chains, and citation lookups while writing hashed artefact directories (`dist/artifacts/<hash>`).ã€F:calc/derive.pyâ€ L1089-L1473ã€‘
- **Functional unit intensity builder** â€” `calc.derive` ships an `intensity` subcommand and `build_intensity_matrix` helper for generating cross-profile functional unit comparisons consumed by narrative helpers and the UI.ã€F:calc/derive.pyâ€ L1330-L1496ã€‘ã€F:app/lib/narratives.pyâ€ L1-L88ã€‘
- **Figure slicing utilities** â€” `calc.figures` constructs stacked, bubble, and Sankey payloads with consistent metadata, layer ordering, and reference bindings for both UI surfaces.ã€F:calc/figures.pyâ€ L1-L210ã€‘
- **Citation management** â€” `calc.citations` resolves IEEE-formatted references, deduplicates keys, and injects numbered lists into outputs and UI panels.ã€F:calc/citations.pyâ€ L1-L72ã€‘
- **Dash exploration UI** â€” [`app/app.py`](./app/app.py) renders Plotly charts, disclosure copy, NA notices, and references straight from derived artefacts; reduced-motion preferences honour `ACX_REDUCED_MOTION`.ã€F:app/app.pyâ€ L1-L132ã€‘ã€F:app/components/_plotly_settings.pyâ€ L1-L34ã€‘
- **Static React/Vite site** â€” [`site/`](./site) packages a Tailwind-styled React client that consumes the same artefacts, supports preset profiles, and builds with Vite/TypeScript for Cloudflare Pages deployment.ã€F:site/package.jsonâ€ L1-L34ã€‘ã€F:site/src/state/profile/index.tsxâ€ L1-L200ã€‘
- **Cloudflare Pages function** â€” [`functions/carbon-acx/[[path]].ts`](./functions/carbon-acx/[[path]].ts) proxies `/carbon-acx/*` requests, enforces immutable caching for artefacts, and optionally fetches from an upstream origin when `CARBON_ACX_ORIGIN` is set.ã€F:functions/carbon-acx/[[path]].tsâ€ L1-L200ã€‘
- **On-demand compute worker** â€” [`workers/compute`](./workers/compute) exposes `/api/compute` and `/api/health` endpoints that call `calc.service.compute_profile`, normalise overrides, and return fresh figure payloads with dataset version reporting.ã€F:workers/compute/index.tsâ€ L1-L104ã€‘ã€F:calc/service.pyâ€ L296-L414ã€‘
- **Artifact packaging workflow** â€” `scripts/package_artifacts.py` and `scripts.prepare_pages_bundle.py` copy whitelisted JSON/CSV/TXT outputs, emit a file index, and write Cloudflare `_headers`/`_redirects` metadata; `tools/sbom.py` produces a CycloneDX SBOM for releases.ã€F:scripts/package_artifacts.pyâ€ L1-L160ã€‘ã€F:scripts/prepare_pages_bundle.pyâ€ L1-L82ã€‘ã€F:tools/sbom.pyâ€ L1-L120ã€‘
- **Data import/export tooling** â€” `scripts/import_csv_to_db.py`, `scripts/export_db_to_csv.py`, and `db/schema.sql` keep SQL backends aligned with canonical CSVs while preserving schema constraints and placeholder guardrails.ã€F:scripts/import_csv_to_db.pyâ€ L1-L160ã€‘ã€F:db/schema.sqlâ€ L1-L80ã€‘

---

## Repository structure

| Path | Purpose |
| --- | --- |
| `calc/` | Core calculation package: schema definitions, datastore interfaces, emission derivation, figure slicing, citation handling, UI theming, API helpers, and upstream metadata utilities.ã€F:calc/schema.pyâ€ L1-L101ã€‘ã€F:calc/service.pyâ€ L1-L120ã€‘ |
| `app/` | Dash development client with Plotly component builders, disclosure panels, intensity narratives, and reference rendering helpers.ã€F:app/app.pyâ€ L1-L132ã€‘ã€F:app/componentsâ€ L1-L120ã€‘ |
| `site/` | Static site sources (React/TypeScript, Tailwind CSS, Vite config, assets) used by `npm run build` and Cloudflare Pages deployment.ã€F:site/package.jsonâ€ L1-L34ã€‘ã€F:site/vite.config.tsâ€ L1-L80ã€‘ |
| `data/` | Canonical CSV datasets (activities, schedules, emission factors, grid intensity, profiles, units, and sources). `_staged/` holds raw extracts pending ingestion.ã€F:data/activities.csvâ€ L1-L40ã€‘ã€F:data/_stagedâ€ L1-L10ã€‘ |
| `scripts/` | Build orchestration scripts for rendering the static site, packaging artefacts, syncing layer catalogues, auditing coverage, and diagnosing deployments.ã€F:scripts/build_site.pyâ€ L1-L120ã€‘ã€F:scripts/dev_diag.shâ€ L1-L16ã€‘ |
| `functions/` | Cloudflare Pages function for proxying `/carbon-acx` traffic and serving artefacts with strict caching semantics.ã€F:functions/carbon-acx/[[path]].tsâ€ L1-L200ã€‘ |
| `workers/` | Cloudflare Worker runtime for on-demand compute endpoints backed by the `calc.service` module.ã€F:workers/compute/index.tsâ€ L1-L104ã€‘ |
| `db/` | SQLite schema for durable backends and parity testing.ã€F:db/schema.sqlâ€ L1-L80ã€‘ |
| `docs/` | Operational guides covering methodology, maintenance cadence, deployment runbooks, routing, testing notes, and contribution traceability.ã€F:docs/MAINTENANCE_CALENDAR.mdâ€ L1-L80ã€‘ã€F:docs/routes.mdâ€ L1-L120ã€‘ |
| `tests/` | Pytest suite including integration tests, backend parity checks, figure regression tests, UI snapshot expectations, and worker/API tests.ã€F:tests/test_backend_parity.pyâ€ L1-L47ã€‘ã€F:tests/ui/test_theme_smoke.pyâ€ L1-L80ã€‘ |
| `tools/` | Developer utilities such as CycloneDX SBOM generation.ã€F:tools/sbom.pyâ€ L1-L120ã€‘ |
| `Makefile` | Primary task runner encapsulating install, lint, test, build, packaging, SBOM, DB import/export, and release placeholders.ã€F:Makefileâ€ L1-L80ã€‘ |
| `pyproject.toml` | Poetry configuration specifying runtime dependencies, extras, linting settings, and build metadata.ã€F:pyproject.tomlâ€ L1-L45ã€‘ |

Each module is documented with inline comments and docstrings; start with `calc/derive.py` to understand the data flow, then explore `app/components` and `site/src` to see how artefacts are consumed.

---

## Installation & setup

### Local development

1. **Install prerequisites**
   - Python 3.11+
   - [Poetry 1.8](https://python-poetry.org/) (CI installs automatically; local developers should install manually)
   - GNU `make`
   - Node.js 18+ (for the `site/` Vite build)
2. **Bootstrap the Python environment**
   ```bash
   make install
   ```
   This installs runtime and development dependencies (ruff, black, pytest, pip-audit, kaleido, Pillow).
3. **Regenerate derived artefacts**
   ```bash
   make build
   ```
   The command runs `python -m calc.derive export` and `python -m calc.derive intensity`, producing hashed builds under `dist/artifacts/<hash>` and updating `dist/artifacts/latest-build.json`.
4. **Sync the layer catalogue for the site**
   ```bash
   make site_install
   make site_build
   ```
   The `site_build` target compiles the React client and copies assets into `dist/site/`.

### Database-backed workflows

- Initialise a SQLite database for parity testing or production experiments:
  ```bash
  make db_init
  make db_import  # loads CSV data into acx.db using schema constraints
  ```
- Run derivations against SQLite or DuckDB by overriding the backend:
  ```bash
  make build-backend B=sqlite
  make build-backend B=duckdb  # requires poetry install --extras db
  ```
  `ACX_DB_PATH` controls the database path when using the SQL-backed store.ã€F:calc/dal/__init__.pyâ€ L67-L90ã€‘

### Front-end toolchain

- Install Node dependencies once (`make site_install`).
- Start a hot-reload development server:
  ```bash
  make site_dev
  ```
  The Vite dev server honours `PUBLIC_BASE_PATH` for nested deployments and serves from `http://localhost:5173` (or the port printed in the console).ã€F:site/vite.config.tsâ€ L1-L80ã€‘

### Production preparation

To mirror the CI/CD pipeline locally:

```bash
make build site package
```

- `make site` renders the static bundle into `dist/site/`.
- `make package` copies distributable artefacts into `dist/packaged-artifacts/`, writes a file index, and prepares Cloudflare metadata.
- `make sbom` emits a CycloneDX SBOM at `dist/sbom/cyclonedx.json` for release automation.

---

## Usage

### Derivation CLI

All derived outputs originate from the `calc.derive` module:

```bash
PYTHONPATH=. poetry run python -m calc.derive export \
  --output-root dist/artifacts \
  --backend csv  # optional: csv (default), sqlite, or duckdb
```

Key outputs within the selected root (`dist/artifacts/<hash>/calc/outputs`):

- `export_view.csv` / `export_view.json` â€” tabular emissions dataset with metadata header comments.
- `figures/{stacked,bubble,sankey}.json` â€” Plotly payloads trimmed for client use.
- `references/*_refs.txt` â€” IEEE-formatted reference lists for each figure.
- `manifest.json` â€” Snapshot metadata (build hash, generated timestamp, layer coverage, regional vintages, citation keys, dependency chain hashes).

`ACX_DATA_BACKEND` controls the datastore backend (`csv` by default; `duckdb` requires the optional dependency). `ACX_OUTPUT_ROOT` and the `--output-root` flag determine where hashed artefacts are written; `ACX_ALLOW_OUTPUT_RM=1` bypasses path safety checks when directing outputs outside `dist/artifacts`.ã€F:calc/derive.pyâ€ L292-L382ã€‘ã€F:calc/derive.pyâ€ L1600-L1658ã€‘

### Intensity matrix CLI

Generate functional-unit intensity slices that power narrative comparisons and downloadable CSVs:

```bash
PYTHONPATH=. poetry run python -m calc.derive intensity \
  --fu all \
  --profile PRO.TO.24_39.HYBRID.2025 \
  --output-dir dist/artifacts
```

Outputs include `intensity_matrix.csv` and `intensity_matrix.json`, which `app/lib/narratives.py` reads to produce comparison blurbs and which the static site surfaces for download.ã€F:calc/derive.pyâ€ L1330-L1496ã€‘ã€F:app/lib/narratives.pyâ€ L1-L120ã€‘

### Dash exploration client

Launch the interactive Dash client after generating artefacts:

```bash
make build
make app  # serves http://localhost:8050
```

The Dash app reads from `calc/outputs` (or a custom `ACX_ARTIFACT_DIR`) and renders:

- Stacked category chart, bubble chart, Sankey flow, and intensity tables using shared Plotly templates and reduced-motion settings.ã€F:app/app.pyâ€ L65-L132ã€‘ã€F:app/components/_plotly_settings.pyâ€ L1-L34ã€‘
- Layer toggles, manifest summaries, and NA notices derived from the export view metadata and `calc.copy_blocks` helpers.ã€F:app/components/disclosure.pyâ€ L1-L160ã€‘ã€F:calc/copy_blocks.pyâ€ L1-L120ã€‘
- Reference sidebar populated via `calc.citations` to guarantee citation parity with the static site.ã€F:app/components/references.pyâ€ L1-L160ã€‘

### Static site bundle

Render and preview the static React site without a live server:

```bash
make package
python -m http.server --directory dist/site 8001
```

The packaged bundle embeds:

- Pre-rendered Plotly HTML snippets for each figure produced by `scripts.build_site`.
- Disclosure panel, manifest summary, and grid vintage table composed from `calc.copy_blocks` and `calc.derive` metadata.ã€F:scripts/build_site.pyâ€ L1-L160ã€‘
- Download links pointing to the packaged artefacts under `dist/site/artifacts/`.
- IEEE reference list identical to the Dash experience.
- Cloudflare Pages metadata (`_headers`, `_redirects`) to configure caching and trailing-slash redirects for `/carbon-acx`.

Deploy the contents of `dist/site/` to Cloudflare Pages; the companion Pages Function under `functions/carbon-acx/[[path]].ts` can proxy `/carbon-acx/*` routes to the hosted bundle or an upstream origin when `CARBON_ACX_ORIGIN` is set.ã€F:functions/carbon-acx/[[path]].tsâ€ L131-L200ã€‘

### Programmatic aggregates

Use the lightweight API to fetch aggregates inside notebooks or downstream services:

```python
from pathlib import Path
from calc.api import get_aggregates

data_dir = Path("data")
config_path = Path("calc/config.yaml")
aggregates, reference_keys = get_aggregates(data_dir, config_path)
print(aggregates.total_annual_emissions_g)
print(reference_keys)
```

`get_aggregates` resolves the active profile, sums annual emissions by activity, and returns the citation keys necessary to build a reference list. `collect_activity_source_keys` is available when you already have derived rows and only need source tracking.ã€F:calc/api.pyâ€ L1-L140ã€‘

### On-demand compute service

`calc.service.compute_profile` mirrors the batch derivation logic for live API contexts: it loads activities, operations, assets, and profiles from any `DataStore`, applies overrides, gathers upstream dependency metadata, and returns trimmed figure payloads with per-layer citations.ã€F:calc/service.pyâ€ L296-L414ã€‘

The Cloudflare Worker in [`workers/compute`](./workers/compute) exposes:

- `GET /api/health` â€” Returns `{ ok: true, dataset: <hash> }` using `ACX_DATASET_VERSION` or SQL row digests.ã€F:workers/compute/index.tsâ€ L70-L104ã€‘ã€F:calc/service.pyâ€ L45-L102ã€‘
- `POST /api/compute` â€” Accepts `{ "profile_id": "...", "overrides": {"ACTIVITY": 1.5} }`, normalises overrides, and returns stacked/bubble/sankey payloads plus manifest metadata and numbered references.ã€F:workers/compute/index.tsâ€ L1-L86ã€‘ã€F:workers/compute/runtime.tsâ€ L1-L120ã€‘

Deploy with Wrangler (`wrangler publish`) after building Python artefacts, or run locally with `wrangler dev` to proxy compute requests against the checked-in demo runtime (`workers/compute/runtime.ts`). Wrangler configuration lives in [`wrangler.toml`](./wrangler.toml).ã€F:wrangler.tomlâ€ L1-L12ã€‘

### Diagnostics & utilities

- `python scripts/audit_layers.py` â€” Generates `artifacts/audit_report.json` summarising layer coverage, missing emission factors, and icon wiring.ã€F:scripts/audit_layers.pyâ€ L1-L160ã€‘
- `python -m scripts.sync_layers_json` â€” Mirrors `data/layers.csv` into `site/public/artifacts/layers.json` for the static client.
- `bash scripts/dev_diag.sh` â€” Fetches layer JSON from local and production endpoints, honouring `PUBLIC_BASE_PATH` and `PAGES_DOMAIN` for quick HTTP debugging.ã€F:scripts/dev_diag.shâ€ L1-L16ã€‘
- `make sbom` â€” Generates `dist/sbom/cyclonedx.json` using `tools/sbom.py` for dependency audits.ã€F:tools/sbom.pyâ€ L1-L120ã€‘

---

## Configuration

### Environment variables

| Variable | Description | Default |
| --- | --- | --- |
| `ACX_DATA_BACKEND` | Selects datastore implementation (`csv`, `duckdb`, or `sqlite`). | `csv` |
| `ACX_DB_PATH` | Overrides the SQLite/DuckDB database path when using SQL backends. | `acx.db` |
| `ACX_OUTPUT_ROOT` | Base directory for hashed artefact builds. | `dist/artifacts` |
| `ACX_ALLOW_OUTPUT_RM` | Set to `1` to allow cleaning arbitrary output directories (bypasses safety checks). | unset |
| `ACX_GENERATED_AT` | Forces the timestamp embedded in figure metadata and manifests. | Current UTC time |
| `ACX_ARTIFACT_DIR` | Points the Dash client at a non-default artefact directory. | `calc/outputs` |
| `ACX_DATASET_VERSION` | Overrides dataset fingerprint exposed by `compute_profile` and the worker health check. | Derived from SQL stats or `dev` in Wrangler configã€F:calc/service.pyâ€ L45-L102ã€‘ã€F:wrangler.tomlâ€ L1-L12ã€‘ |
| `ACX_REDUCED_MOTION` | When truthy, disables Plotly transitions in the Dash client. | unset (motion enabled) |
| `CARBON_ACX_ORIGIN` | Cloudflare Pages function upstream origin for proxying `/carbon-acx/*` requests. | unset (serve local bundle) |
| `PUBLIC_BASE_PATH` | Adjusts static site asset fetch paths and Cloudflare routing helpers. | `/` |
| `PAGES_DOMAIN` | Used by `scripts/dev_diag.sh` to query production bundles. | unset |

### Configuration files & flags

- [`calc/config.yaml`](./calc/config.yaml) â€” Declares the default profile used in figures and manifest summaries.ã€F:calc/config.yamlâ€ L1-L1ã€‘
- [`wrangler.toml`](./wrangler.toml) â€” Configures the compute worker entry point, dataset version default, and Pages output directory.ã€F:wrangler.tomlâ€ L1-L12ã€‘
- CLI flags for `calc.derive export`/`intensity` allow overriding `--backend`, `--db`, `--output-root`, `--functional_unit`, and `--profile` without touching environment variables.ã€F:calc/derive.pyâ€ L1604-L1664ã€‘
- Make targets accept `ACX_DATA_BACKEND` and `OUTPUT_BASE` overrides (`make build ACX_DATA_BACKEND=duckdb`).ã€F:Makefileâ€ L1-L40ã€‘

### Preset data

- `site/public/artifacts` holds development artefacts (layers catalog, demo manifest) used by the Vite dev server; keep this folder in sync with the latest build via `scripts/sync_layers_json.py`.
- `site/src/data/presets.json` defines preset cards displayed in the React client with serialized profile controls and activity overrides. Editing this JSON updates the UI without backend changes.
- `calc/references/*.txt` contains IEEE-formatted citations keyed by `source_id`. Add new references here and reference their keys in CSV data before rebuilding.ã€F:calc/citations.pyâ€ L1-L48ã€‘

---

## Build & deployment

1. **Local build** â€” `make build` â†’ `dist/artifacts/<hash>` â†’ `make site` â†’ `dist/site` â†’ `make package` â†’ `dist/packaged-artifacts` + `_headers/_redirects` ready for Cloudflare Pages.ã€F:Makefileâ€ L1-L80ã€‘ã€F:scripts/prepare_pages_bundle.pyâ€ L1-L82ã€‘
2. **Continuous integration** â€” `.github/workflows/ci.yml` runs YAML linting, Poetry installs, deterministic builds (`make build-static`), uploads artefacts, and executes pytest on Python 3.11.ã€F:.github/workflows/ci.ymlâ€ L1-L67ã€‘
3. **Release automation** â€” `.github/workflows/release.yml` generates SBOMs, installs extras (including DuckDB), and publishes GitHub releases when tags prefixed with `v` are pushed.ã€F:.github/workflows/release.ymlâ€ L1-L41ã€‘
4. **Cloudflare Pages** â€” Deploy `dist/site/` to Pages. The bundled `_headers` enforce immutable caching for `/artifacts/*`; `_redirects` handles `/carbon-acx` canonicalisation. The Pages Function proxies artefact requests, applies CORS headers, and forwards to an upstream origin when configured.ã€F:functions/carbon-acx/[[path]].tsâ€ L1-L200ã€‘
5. **Cloudflare Worker (compute)** â€” Publish `workers/compute` with Wrangler. The Worker is stateless and reads derived artefacts embedded in `workers/compute/runtime.ts` by default; integrate with a persistent SQL backend by bundling `calc.service` into the Worker or exposing the API from a Python runtime behind the function.
6. **Artefact provenance** â€” `dist/artifacts/latest-build.json` points to the most recent build hash. Package the referenced directory verbatim for downstream consumers or audits.ã€F:calc/derive.pyâ€ L1500-L1542ã€‘

---

## Testing & QA

Quality gates are enforced through pytest, ruff, black, SBOM generation, and front-end checks:

- **Python unit & integration tests (`tests/`)**
  - Schema validations, NA segment handling, functional unit formulas, and grid scaling (`tests/test_schema.py`, `tests/test_grid_index.py`).
  - Backend parity ensures CSV, SQLite, and DuckDB exports match byte-for-byte (`tests/test_backend_parity.py`, `tests/test_dal_sql.py`).
  - Static site smoke tests confirm Plotly HTML, asset packaging, and layer syncing succeed (`tests/test_static_site_build.py`, `tests/test_prepare_pages_bundle.py`).
  - Worker/API tests validate `compute_profile` payloads and override handling (`tests/test_api_compute.py`, `tests/test_service.py`).
  - UI theming and visual fixtures assert Plotly traces and intensity narratives remain stable (`tests/ui/test_theme_smoke.py`, `tests/visual/test_visual_smoke.py`).
- **Static analysis**
  - `make lint` runs ruff and black (100-character line length) across Python modules.ã€F:Makefileâ€ L1-L40ã€‘
  - `.yamllint` ensures workflow YAML hygiene via the CI `lint-yaml` job.ã€F:.github/workflows/ci.ymlâ€ L1-L26ã€‘
- **Front-end tests**
  - `npm run test` in `site/` executes Vitest suites defined alongside React components (`site/src/routes/__tests__`).
- **Dependency governance**
  - `make sbom` generates a CycloneDX SBOM (`dist/sbom/cyclonedx.json`) for dependency auditing; `pip-audit` is available via Poetry dev dependencies.ã€F:tools/sbom.pyâ€ L1-L120ã€‘

Before submitting a change, run:

```bash
make format lint test build package
(cd site && npm test)
```

---

## Contributing guide

Carbon ACX follows a conventional GitHub workflow (see [`CONTRIBUTING.md`](./CONTRIBUTING.md)):

1. Fork or branch from `main`.
2. Use descriptive branch names (e.g. `feature/derived-metrics`).
3. Keep commits focused; include context in commit messages about data updates and methodology changes. Commit messages should describe the dataset slice touched, references added, and any schema adjustments.
4. Run `make validate` (lint + tests), `make build`, and relevant packaging steps locally; run `npm run build` in `site/` when touching front-end code.
5. Open a pull request with:
   - Summary of changes and impacted artefacts (hash directories, CSV deltas, figure updates).
   - Confirmation that linting, tests, and builds passed (Python + Node where applicable).
   - Notes about new data sources or reference files (include provenance in the PR description as outlined in `docs/CONTRIBUTING_SERIES.md`).ã€F:docs/CONTRIBUTING_SERIES.mdâ€ L1-L120ã€‘
6. Expect code review on data hygiene, reproducibility, accessibility, and reference integrity. Changes touching `data/` or `calc/references/` must document provenance and update audit scripts if necessary.

Coding standards:

- Prefer Pydantic models for new datasets and extend `calc/schema.py` rather than bypassing validation.
- Avoid catching broad exceptions; propagate validation errors so the derivation pipeline fails fast.
- Keep Plotly figure builders deterministic (sorted keys, explicit colour ordering, shared templates via `calc.ui.theme`).ã€F:calc/ui/theme.pyâ€ L1-L120ã€‘
- Never commit generated artefacts (`build/`, `dist/`, `calc/outputs/`, `site/dist`).
- Follow the serial/traceability guidance in `docs/CONTRIBUTING_SERIES.md` when referencing ACX specifications.

---

## Security & compliance

- **Data isolation** â€” Derived outputs are written to hashed directories and cleared only within guardrails to avoid deleting arbitrary paths (`calc.derive.is_safe_output_dir`).ã€F:calc/derive.pyâ€ L292-L336ã€‘
- **Dependency governance** â€” Dependencies are pinned via Poetry and npm lockfiles; `tools/sbom.py` emits CycloneDX SBOMs and `pip-audit` is available for vulnerability scanning.ã€F:tools/sbom.pyâ€ L1-L120ã€‘ã€F:site/package-lock.jsonâ€ L1-L40ã€‘
- **Runtime posture** â€” Dash, the static site, and the Cloudflare Worker read precomputed bundles only; no live data collection occurs at runtime. The Cloudflare Pages Function proxies traffic without persisting secrets server-side and enforces immutable caching headers for artefacts.ã€F:functions/carbon-acx/[[path]].tsâ€ L1-L200ã€‘
- **Access control** â€” The compute Worker accepts only JSON POST payloads, normalises overrides, and rejects invalid types before invoking `calc.service.compute_profile`, limiting injection risks.ã€F:workers/compute/index.tsâ€ L1-L86ã€‘
- **Licensing** â€” The project is released under the MIT License (`LICENSE`). Include source acknowledgements when adding new references or datasets.
- **Compliance placeholders** â€” SBOM generation and release workflows lay groundwork for future compliance processes; integration with vulnerability management tools can consume `dist/sbom/cyclonedx.json`.

---

## Roadmap

| Status | Milestone | Description |
| --- | --- | --- |
| âœ… | Immutable data builds | Content-hashed artefact directories with manifest pointers, dependency chains, and reference parity across clients.ã€F:calc/derive.pyâ€ L1474-L1534ã€‘ |
| âœ… | Multi-channel delivery | Dash exploration client, static React/Vite bundle, Pages Function, and Worker API share derived Plotly payloads and disclosure copy.ã€F:app/app.pyâ€ L65-L132ã€‘ã€F:site/src/components/VizCanvas.tsxâ€ L1-L160ã€‘ |
| âœ… | Backend parity | CSV, DuckDB, and SQLite datastores validated via automated parity tests to ensure consistent exports.ã€F:tests/test_backend_parity.pyâ€ L1-L47ã€‘ã€F:tests/test_dal_sql.pyâ€ L1-L120ã€‘ |
| âœ… | Intensity narratives | Functional-unit intensity builder feeds UI narratives and CSV downloads, keeping comparisons aligned across channels.ã€F:calc/derive.pyâ€ L1330-L1496ã€‘ã€F:app/lib/narratives.pyâ€ L1-L160ã€‘ |
| ğŸš§ | Release automation | `make release` placeholder to be replaced with scripted tagging, changelog updates (`docs/CHANGELOG.md`), asset uploads, and Pages deployments. |
| ğŸš§ | Dataset refresh tooling | Extend `docs/MAINTENANCE_CALENDAR.md` with automation for pulling `_staged/` data into `data/` while preserving provenance metadata.ã€F:docs/MAINTENANCE_CALENDAR.mdâ€ L1-L80ã€‘ |
| ğŸš§ | Worker dataset integration | Replace the demo dataset in `workers/compute/runtime.ts` with generated artefacts or live `calc.service` bindings and document deployment playbooks. |
| ğŸ§­ | Additional datastore backends | Explore further `DataStore` implementations (e.g. Postgres) leveraging the existing abstraction in `calc.dal` and `calc.dal_sql`. |
| ğŸ§­ | Expanded visualisations | Prototype intensity waterfalls, cohort comparisons, or sensitivity analyses using the figure slicing framework in `calc.figures`. |
| ğŸ§­ | Live API surface | Wrap `calc.api` aggregates and `compute_profile` in an authenticated HTTP service for downstream integrations once security requirements are defined. |

Legend: âœ… implemented Â· ğŸš§ in-progress or partially scaffolded Â· ğŸ§­ planned/under evaluation.

---

## FAQ & troubleshooting

**Why does `calc/outputs` stay empty?**  Artefacts are written to hashed directories under `dist/artifacts/<hash>`. Use `scripts/_artifact_paths.resolve_artifact_outputs` or inspect `dist/artifacts/latest-build.json` to locate the latest build.ã€F:calc/derive.pyâ€ L1500-L1542ã€‘

**`python -m calc.derive export` refuses to clear my output directory.**  The pipeline protects against deleting unintended paths. Either point `--output-root` inside `dist/artifacts` or set `ACX_ALLOW_OUTPUT_RM=1` when you are sure the target is safe.ã€F:calc/derive.pyâ€ L292-L336ã€‘

**The Dash app cannot find figures.**  Ensure `make build` ran successfully and `ACX_ARTIFACT_DIR` (if set) points to the directory containing `figures/*.json` and `manifest.json`. Remember that Dash defaults to `calc/outputs`, while `make build` writes hashed outputs into `dist/artifacts/<hash>/calc/outputs`.

**How do I add a new reference?**  Drop an IEEE-formatted text file in `calc/references/` and reference its stem in emission factors or grid intensity rows. Run `make build` to propagate the citation into artefacts and UI reference panels.ã€F:calc/citations.pyâ€ L1-L72ã€‘

**Plotly figures look different locally vs CI.**  Confirm `kaleido` is installed (via `make install`) and avoid locale-dependent formatting. Re-run `make build` to regenerate Plotly JSON with consistent ordering.

**Node build fails with an engine warning.**  Vite 5 requires Node.js â‰¥ 18. Upgrade Node locally or use the version pinned in `.nvmrc` if present.

**Cloudflare Pages serves HTML for JSON routes.**  Ensure the Pages Function is deployed alongside the static site and that `_headers`/`_redirects` shipped with the bundle remain intact (`make package` handles this automatically). `CARBON_ACX_ORIGIN` should include an absolute origin without trailing slash.

**Worker API returns demo data only.**  The checked-in Worker runtime includes a small demo dataset. Replace `workers/compute/runtime.ts` with generated artefacts or adapt the Worker to call a hosted `compute_profile` endpoint for production use.

**Debugging data loads.**  Use `scripts/dev_diag.sh` to compare artefact headers locally and in production. The script respects `PUBLIC_BASE_PATH` so you can verify both the static bundle (`http://127.0.0.1:4173`) and your Pages domain return JSON rather than the SPA fallback.ã€F:scripts/dev_diag.shâ€ L1-L16ã€‘

---

## References

- [`CONTRIBUTING.md`](./CONTRIBUTING.md)
- [`docs/CHANGELOG.md`](./docs/CHANGELOG.md)
- [`docs/MAINTENANCE_CALENDAR.md`](./docs/MAINTENANCE_CALENDAR.md)
- [`docs/ONLINE_METHOD_NOTES.md`](./docs/ONLINE_METHOD_NOTES.md)
- [`docs/WHAT_RUNS_WHERE.md`](./docs/WHAT_RUNS_WHERE.md)
- [`docs/TESTING_NOTES.md`](./docs/TESTING_NOTES.md)
- [`docs/deploy.md`](./docs/deploy.md)
- [`docs/routes.md`](./docs/routes.md)
- [`LICENSE`](./LICENSE)

---

_Note: To satisfy repo hygiene tests, avoid using the contiguous token spelled â€œF a s t A P Iâ€ in docs._

### Serials & Traceability

Refer to [`docs/CONTRIBUTING_SERIES.md`](./docs/CONTRIBUTING_SERIES.md) for guidance on citing ACX specifications, CDX prompts, and the PR lineage required for every contribution.
