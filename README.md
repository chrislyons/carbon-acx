# Carbon ACX

Carbon ACX is an open reference stack for building trustworthy carbon accounting datasets and presenting them as polished digital experiences. It shows how to move from raw activity records to public-ready disclosures without losing transparency along the way.

---

## Table of contents

1. [Overview](#overview)
   - [In plain language](#in-plain-language)
   - [What you get](#what-you-get)
   - [Who uses Carbon ACX](#who-uses-carbon-acx)
2. [How it works](#how-it-works)
3. [Repository tour](#repository-tour)
4. [Getting started](#getting-started)
   - [Quick demo without coding](#quick-demo-without-coding)
   - [Developer setup](#developer-setup)
   - [Database-backed workflows](#database-backed-workflows)
   - [Front-end toolchain](#front-end-toolchain)
5. [Working with the toolkit](#working-with-the-toolkit)
   - [Derivation CLI](#derivation-cli)
   - [Intensity matrix CLI](#intensity-matrix-cli)
   - [Dash exploration client](#dash-exploration-client)
   - [Static site bundle](#static-site-bundle)
   - [Programmatic aggregates](#programmatic-aggregates)
   - [On-demand compute service](#on-demand-compute-service)
   - [Diagnostics & utilities](#diagnostics--utilities)
6. [Configuration](#configuration)
   - [Environment variables](#environment-variables)
   - [Configuration files & flags](#configuration-files--flags)
   - [Preset data](#preset-data)
7. [Build & deployment](#build--deployment)
8. [Testing & QA](#testing--qa)
9. [Contributing guide](#contributing-guide)
10. [Security & compliance](#security--compliance)
11. [Roadmap](#roadmap)
12. [FAQ & troubleshooting](#faq--troubleshooting)
13. [References](#references)
14. [Serials & traceability](#serials--traceability)
15. [Architecture extensions](#architecture-extensions)
    - [ACX041 View Provenance Module](#acx041-view-provenance-module)

---

## Overview

### In plain language

Carbon ACX bundles three things:

1. **A curated demo dataset** that reflects common professional-service, digital, and light-industrial activities. It includes emission factors, grid intensities, schedules, and preset profiles that can be rebuilt on demand from the CSV files in `data/`.
2. **A reproducible calculation engine** that converts those inputs into emissions results. The `calc` Python package validates every record, joins the right factors, computes totals, and writes the outputs into versioned folders so you always know exactly what changed between runs.ã€F:calc/schema.pyâ€ L1-L167ã€‘ã€F:calc/derive.pyâ€ L1089-L1473ã€‘
3. **Two ready-to-ship experiences**â€”a Dash web app and a static React siteâ€”plus a Cloudflare Worker API. All of them read the same derived artefacts so audiences get identical numbers regardless of the channel.ã€F:app/app.pyâ€ L1-L132ã€‘ã€F:site/src/App.tsxâ€ L1-L58ã€‘ã€F:workers/compute/index.tsâ€ L1-L104ã€‘

The result is a transparent example of how to publish carbon disclosures without spreadsheets, with a workflow business stakeholders and engineers can both follow.

### What you get

- **Trustworthy data pipeline** â€“ strict validation, reference tracking, and manifest metadata keep every build auditable.ã€F:calc/schema.pyâ€ L1-L167ã€‘ã€F:calc/derive.pyâ€ L1474-L1542ã€‘
- **Interactive storytelling** â€“ Plotly-based Dash components and a Tailwind-powered React site render the same charts, tables, and disclosure copy for presentations or public sites, now with focus-managed stage navigation, an agency contribution strip, and two-stage Sankey overlays for layered scenarios.ã€F:app/components/_plotly_settings.pyâ€ L1-L40ã€‘ã€F:app/components/agency_strip.pyâ€ L1-L52ã€‘ã€F:app/components/sankey.pyâ€ L17-L113ã€‘ã€F:site/src/components/LayerBrowser.tsxâ€ L77-L145ã€‘ã€F:site/src/components/VizCanvas.tsxâ€ L1-L160ã€‘
- **Automation hooks** â€“ CLI commands, Make targets, and a Cloudflare Worker make it easy to integrate the dataset into CI/CD or downstream services.ã€F:Makefileâ€ L1-L80ã€‘ã€F:workers/compute/runtime.tsâ€ L1-L120ã€‘
- **Scenario depth** â€“ Refrigerant operations, embodied defence manufacturing, private security overlays, and new civilian aviation pathways expand the dataset for stress-testing optional layer toggles and disclosures.ã€F:data/activities.csvâ€ L44-L84ã€‘ã€F:data/layers.csvâ€ L1-L16ã€‘

### Who uses Carbon ACX

- **Sustainability leads** exploring how structured carbon reporting can be shared across teams and clients.
- **Data practitioners** who want a reproducible reference for schema design, derivations, and manifest-driven releases.
- **Front-of-house teams** (communications, design, investor relations) looking for a polished, jargon-light experience they can embed in sites or decks.

---

## How it works

Carbon ACX follows a â€œderive once, serve anywhereâ€ flow:

1. **Collect & validate** â€“ CSV inputs are described with Pydantic models that enforce units, scopes, and functional-unit formulas.ã€F:calc/schema.pyâ€ L1-L167ã€‘
2. **Derive & package** â€“ `python -m calc.derive` computes annual emissions, intensity matrices, references, and manifest metadata, then writes a content-hashed bundle under `dist/artifacts/<hash>`.ã€F:calc/derive.pyâ€ L1089-L1526ã€‘
3. **Publish & reuse** â€“ Dash (`app/`), the static site (`site/`), the Cloudflare Pages Function, and the compute Worker all read from the same bundle so numbers stay in sync across every surface.ã€F:app/app.pyâ€ L1-L132ã€‘ã€F:site/vite.config.tsâ€ L1-L80ã€‘ã€F:functions/carbon-acx/[[path]].tsâ€ L1-L200ã€‘ã€F:workers/compute/index.tsâ€ L1-L104ã€‘

---

## Repository tour

| Path | Purpose |
| --- | --- |
| `calc/` | Core Python package: schema validation, datastore abstraction, derivation logic, Plotly figure builders, citation helpers, and API utilities.ã€F:calc/schema.pyâ€ L1-L101ã€‘ã€F:calc/service.pyâ€ L1-L120ã€‘ |
| `app/` | Dash development client with reusable components, disclosure panels, and reduced-motion aware Plotly templates.ã€F:app/app.pyâ€ L1-L132ã€‘ã€F:app/components/_plotly_settings.pyâ€ L1-L40ã€‘ |
| `site/` | Static React/Vite site styled with Tailwind that consumes derived artefacts and mirrors the Dash experience.ã€F:site/package.jsonâ€ L1-L34ã€‘ã€F:site/src/routes/Story.tsxâ€ L1-L160ã€‘ |
| `data/` | Canonical CSV datasets for activities, schedules, emission factors, grid intensities, profiles, units, and sources; `_staged/` stores raw extracts awaiting ingestion.ã€F:data/activities.csvâ€ L1-L40ã€‘ |
| `scripts/` | Build, packaging, audit, and diagnostic helpers for artefacts, layer catalogues, and deployment automation.ã€F:scripts/prepare_pages_bundle.pyâ€ L1-L82ã€‘ã€F:scripts/audit_layers.pyâ€ L1-L160ã€‘ |
| `functions/` | Cloudflare Pages Function that serves the static bundle, proxies artefact requests, and applies immutable caching headers.ã€F:functions/carbon-acx/[[path]].tsâ€ L1-L200ã€‘ |
| `workers/` | Cloudflare Worker exposing a JSON API that calls `calc.service.compute_profile` for live figure generation.ã€F:workers/compute/index.tsâ€ L1-L104ã€‘ |
| `db/` | SQLite schema used for parity tests and optional persistent backends.ã€F:db/schema.sqlâ€ L1-L80ã€‘ |
| `docs/` | Operational guides, deployment notes, testing references, and maintenance calendars.ã€F:docs/routes.mdâ€ L1-L120ã€‘ |
| `tests/` | Pytest suite plus UI snapshot checks covering schema guardrails, backend parity, and packaging workflows.ã€F:tests/test_backend_parity.pyâ€ L1-L47ã€‘ã€F:tests/ui/test_theme_smoke.pyâ€ L1-L80ã€‘ |
| `tools/` | Developer utilities such as CycloneDX SBOM generation.ã€F:tools/sbom.pyâ€ L1-L120ã€‘ |
| `Makefile` | Main entry points for install, lint, test, build, packaging, and SBOM tasks.ã€F:Makefileâ€ L1-L80ã€‘ |
| `pyproject.toml` | Poetry configuration with runtime dependencies and optional extras (e.g. DuckDB support).ã€F:pyproject.tomlâ€ L1-L45ã€‘ |

---

## Getting started

### Quick demo without coding

1. Clone the repository and ensure Python 3.11+ and Node.js 18+ are available.
2. Install the Python dependencies and build the artefacts:
   ```bash
   make install
   make build
   ```
3. Package the static site and preview it in your browser:
   ```bash
   make package
   python -m http.server --directory dist/site 8001
   ```
   Visit `http://localhost:8001` to click through the experience produced from the latest data bundle. The Dash app can be started with `make app` if you prefer an interactive notebook-style exploration.ã€F:Makefileâ€ L1-L80ã€‘ã€F:app/app.pyâ€ L65-L132ã€‘

### Developer setup

1. **Install prerequisites**
   - Python 3.11+
   - [Poetry 1.8](https://python-poetry.org/)
   - GNU `make`
   - Node.js 18+ (for the Vite build in `site/`)
2. **Bootstrap the Python environment**
   ```bash
   make install
   ```
   This installs runtime and development dependencies including ruff, black, pytest, kaleido, Pillow, and pip-audit.ã€F:Makefileâ€ L1-L40ã€‘
3. **Regenerate derived artefacts**
   ```bash
   make build
   ```
   The command runs `python -m calc.derive export` and `python -m calc.derive intensity`, writes hashed outputs to `dist/artifacts/<hash>`, and updates `dist/artifacts/latest-build.json`.ã€F:calc/derive.pyâ€ L1474-L1542ã€‘
4. **Sync the front-end assets**
   ```bash
   make site_install
   make site_build
   ```
   These targets install Node dependencies, compile the React client, and copy artefacts into `dist/site/` for local preview.ã€F:Makefileâ€ L41-L80ã€‘

### Database-backed workflows

- Initialise a SQLite database for parity testing or experiments:
  ```bash
  make db_init
  make db_import
  ```
- Build using a specific backend:
  ```bash
  make build-backend B=sqlite
  make build-backend B=duckdb  # requires poetry install --extras db
  ```
  Set `ACX_DB_PATH` to control the path when using SQL-backed stores.ã€F:calc/dal/__init__.pyâ€ L67-L90ã€‘

### Front-end toolchain

- Install Node dependencies once: `make site_install`.
- Start the hot-reload dev server:
  ```bash
  make site_dev
  ```
  Vite honours `PUBLIC_BASE_PATH` for nested deployments and serves from `http://localhost:5173` by default.ã€F:site/vite.config.tsâ€ L1-L80ã€‘

---

## Working with the toolkit

### Derivation CLI

All derived outputs originate from the `calc.derive` module:

```bash
PYTHONPATH=. poetry run python -m calc.derive export \
  --output-root dist/artifacts \
  --backend csv  # optional: csv (default), sqlite, or duckdb
```

Key outputs inside `dist/artifacts/<hash>/calc/outputs` include:

- `export_view.{csv,json}` â€“ tabular emissions dataset with metadata headers.
- `figures/{stacked,bubble,sankey}.json` â€“ Plotly payloads consumed by Dash and the static site.
- `references/*_refs.txt` â€“ IEEE-formatted reference lists.
- `manifest.json` â€“ Build hash, generated timestamp, layer coverage, regional vintages, citation keys, and dependency hashes.

Use `ACX_DATA_BACKEND`, `ACX_OUTPUT_ROOT`, and `ACX_ALLOW_OUTPUT_RM` to control where artefacts are written and which datastore backs the run.ã€F:calc/derive.pyâ€ L292-L382ã€‘ã€F:calc/derive.pyâ€ L1600-L1664ã€‘

### Intensity matrix CLI

Generate functional-unit comparisons for narratives and downloads:

```bash
PYTHONPATH=. poetry run python -m calc.derive intensity \
  --fu all \
  --profile PRO.TO.24_39.HYBRID.2025 \
  --output-dir dist/artifacts
```

Outputs include `intensity_matrix.{csv,json}`, which power the narrative helpers in the Dash app and the static site downloads.ã€F:calc/derive.pyâ€ L1330-L1496ã€‘ã€F:app/lib/narratives.pyâ€ L1-L120ã€‘

### Dash exploration client

Launch the interactive Dash client after generating artefacts:

```bash
make build
make app  # serves http://localhost:8050
```

The app loads data from `calc/outputs` (or `ACX_ARTIFACT_DIR`) and renders Plotly charts, manifest summaries, disclosure copy, and reference sidebars with consistent styling and reduced-motion support.ã€F:app/app.pyâ€ L65-L132ã€‘ã€F:app/components/disclosure.pyâ€ L1-L160ã€‘

### Static site bundle

Render and preview the static React site without a live backend:

```bash
make package
python -m http.server --directory dist/site 8001
```

The packaged bundle embeds pre-rendered Plotly HTML, disclosure panels, manifest summaries, and download links pointing to the bundled artefacts. `_headers` and `_redirects` files configure caching and canonical routing for Cloudflare Pages deployments.ã€F:scripts/build_site.pyâ€ L1-L160ã€‘ã€F:functions/carbon-acx/[[path]].tsâ€ L1-L200ã€‘

### Programmatic aggregates

Fetch high-level totals inside notebooks or downstream services:

```python
from pathlib import Path
from calc.api import get_aggregates

data_dir = Path("data")
config_path = Path("calc/config.yaml")
aggregates, reference_keys = get_aggregates(data_dir, config_path)
print(aggregates.total_annual_emissions_g)
print(reference_keys)
```

`get_aggregates` resolves the active profile, sums annual emissions by activity, and returns the citation keys needed for disclosure copy. `collect_activity_source_keys` helps when you already have derived rows and only need source tracking.ã€F:calc/api.pyâ€ L1-L140ã€‘

### On-demand compute service

`calc.service.compute_profile` mirrors the batch derivation logic for live API contexts: it loads data through any `DataStore`, applies overrides, gathers upstream metadata, and returns trimmed figure payloads with per-layer references.ã€F:calc/service.pyâ€ L296-L414ã€‘

The Cloudflare Worker in `workers/compute` exposes:

- `GET /api/health` â€“ Returns `{ ok: true, dataset: <hash> }` based on the dataset version fingerprint.ã€F:workers/compute/index.tsâ€ L70-L104ã€‘ã€F:calc/service.pyâ€ L45-L102ã€‘
- `POST /api/compute` â€“ Accepts profile selections and overrides, then responds with stacked/bubble/sankey payloads, manifest metadata, and numbered references.ã€F:workers/compute/index.tsâ€ L1-L86ã€‘ã€F:workers/compute/runtime.tsâ€ L1-L120ã€‘

Deploy with Wrangler (`wrangler publish`) or run locally with `wrangler dev`. Configuration lives in `wrangler.toml`.ã€F:wrangler.tomlâ€ L1-L12ã€‘

### Diagnostics & utilities

- `python scripts/audit_layers.py` â€“ Generates `artifacts/audit_report.json` summarising layer coverage, missing emission factors, and icon wiring.ã€F:scripts/audit_layers.pyâ€ L1-L160ã€‘
- `python -m scripts.sync_layers_json` â€“ Mirrors `data/layers.csv` into `site/public/artifacts/layers.json` for the static client.
- `bash scripts/dev_diag.sh` â€“ Compares artefact headers locally and remotely, respecting `PUBLIC_BASE_PATH` and `PAGES_DOMAIN` for quick HTTP debugging.ã€F:scripts/dev_diag.shâ€ L1-L16ã€‘
- `make sbom` â€“ Produces `dist/sbom/cyclonedx.json` using `tools/sbom.py` for release compliance.ã€F:tools/sbom.pyâ€ L1-L120ã€‘

---

## Configuration

### Environment variables

| Variable | Description | Default |
| --- | --- | --- |
| `ACX_DATA_BACKEND` | Select datastore implementation (`csv`, `duckdb`, or `sqlite`). | `csv` |
| `ACX_DB_PATH` | Override the SQLite/DuckDB database path when using SQL backends. | `acx.db` |
| `ACX_OUTPUT_ROOT` | Base directory for hashed artefact builds. | `dist/artifacts` |
| `ACX_ALLOW_OUTPUT_RM` | Set to `1` to allow cleaning arbitrary output directories (bypasses safety checks). | unset |
| `ACX_GENERATED_AT` | Force the timestamp embedded in figure metadata and manifests. | Current UTC time |
| `ACX_ARTIFACT_DIR` | Point the Dash client at a non-default artefact directory. | `calc/outputs` |
| `ACX_DATASET_VERSION` | Override the dataset fingerprint exposed by `compute_profile` and worker health checks. | Derived from SQL stats or `dev` in Wrangler configã€F:calc/service.pyâ€ L45-L102ã€‘ã€F:wrangler.tomlâ€ L1-L12ã€‘ |
| `ACX_REDUCED_MOTION` | When truthy, disables Plotly transitions in the Dash client. | unset |
| `CARBON_ACX_ORIGIN` | Cloudflare Pages Function upstream origin for proxying `/carbon-acx/*` routes. | unset |
| `PUBLIC_BASE_PATH` | Adjust static site asset fetch paths and Cloudflare routing helpers. | `/` |
| `PAGES_DOMAIN` | Used by `scripts/dev_diag.sh` to query production bundles. | unset |

### Configuration files & flags

- `calc/config.yaml` â€“ Declares the default profile used in figures and manifest summaries.ã€F:calc/config.yamlâ€ L1-L1ã€‘
- `wrangler.toml` â€“ Configures the compute worker entry point, dataset version default, and Pages output directory.ã€F:wrangler.tomlâ€ L1-L12ã€‘
- CLI flags for `calc.derive export` and `calc.derive intensity` allow overriding the backend, database, output root, functional unit, and profile without touching environment variables.ã€F:calc/derive.pyâ€ L1600-L1664ã€‘
- Make targets accept `ACX_DATA_BACKEND` and `OUTPUT_BASE` overrides (e.g. `make build ACX_DATA_BACKEND=duckdb`).ã€F:Makefileâ€ L1-L40ã€‘

### Preset data

- `site/public/artifacts` holds development artefacts (layers catalogue, demo manifest) used by the Vite dev server. Regenerate via `scripts/sync_layers_json.py` after updating CSVs.
- `site/src/data/presets.json` defines preset cards displayed in the React client with serialised profile controls and overrides.
- `calc/references/*.txt` stores IEEE-formatted citations keyed by `source_id`. Reference these keys in CSV data before rebuilding.ã€F:calc/citations.pyâ€ L1-L48ã€‘

---

## Build & deployment

1. **Local build** â€“ `make build` â†’ `dist/artifacts/<hash>` â†’ `make site` â†’ `dist/site` â†’ `make package` â†’ `dist/packaged-artifacts` plus `_headers/_redirects` ready for Cloudflare Pages.ã€F:Makefileâ€ L1-L80ã€‘ã€F:scripts/prepare_pages_bundle.pyâ€ L1-L82ã€‘
2. **Continuous integration** â€“ `.github/workflows/ci.yml` installs Poetry, runs deterministic builds (`make build-static`), uploads artefacts, and executes pytest and lint checks.ã€F:.github/workflows/ci.ymlâ€ L1-L67ã€‘
3. **Release automation** â€“ `.github/workflows/release.yml` generates SBOMs, installs extras (including DuckDB), and publishes GitHub releases for `v*` tags.ã€F:.github/workflows/release.ymlâ€ L1-L41ã€‘
4. **Cloudflare Pages** â€“ Deploy `dist/site/` to Pages. `_headers` enforce immutable caching for `/artifacts/*`, `_redirects` handles canonical routing, and the Pages Function proxies `/carbon-acx/*` traffic when `CARBON_ACX_ORIGIN` is configured.ã€F:functions/carbon-acx/[[path]].tsâ€ L1-L200ã€‘
5. **Cloudflare Worker (compute)** â€“ Publish `workers/compute` with Wrangler. The Worker ships with an embedded demo dataset and can call `calc.service` when bundled with Python artefacts or exposed via an upstream API.
6. **Artefact provenance** â€“ `dist/artifacts/latest-build.json` points to the most recent build hash; package that directory verbatim for downstream consumers or audits.ã€F:calc/derive.pyâ€ L1474-L1542ã€‘

---

## Testing & QA

Carbon ACX enforces quality through automated testing and reproducible builds:

- **Python unit & integration tests (`tests/`)**
  - Schema validations, NA segment handling, and grid scaling checks (`tests/test_schema.py`, `tests/test_grid_index.py`).
  - Backend parity for CSV, SQLite, and DuckDB exports (`tests/test_backend_parity.py`, `tests/test_dal_sql.py`).
  - Static site smoke tests for Plotly HTML, asset packaging, and layer syncing (`tests/test_static_site_build.py`, `tests/test_prepare_pages_bundle.py`).
  - Worker/API tests for `compute_profile` payloads and override handling (`tests/test_api_compute.py`, `tests/test_service.py`).
  - UI theming and visual fixtures for Plotly traces and intensity narratives (`tests/ui/test_theme_smoke.py`, `tests/visual/test_visual_smoke.py`).
- **Static analysis** â€“ `make lint` runs ruff and black (100-character line length). `.yamllint` keeps workflow YAML clean.ã€F:Makefileâ€ L1-L40ã€‘ã€F:.github/workflows/ci.ymlâ€ L1-L26ã€‘
- **Front-end tests** â€“ `npm run test` in `site/` executes Vitest suites co-located with React components.
- **Dependency governance** â€“ `make sbom` generates a CycloneDX SBOM (`dist/sbom/cyclonedx.json`) and pip-audit ships with the Poetry dev dependencies.ã€F:tools/sbom.pyâ€ L1-L120ã€‘

Before opening a pull request, run:

```bash
make format lint test build package
(cd site && npm test)
```

---

## Contributing guide

Carbon ACX uses a conventional GitHub workflow (see `CONTRIBUTING.md` for full details):

1. Branch from `main` with a descriptive name (e.g. `feature/derived-metrics`).
2. Keep commits focused and document dataset slices, references, and schema adjustments in messages.
3. Run `make validate` (lint + tests), `make build`, and relevant packaging steps locally; run `npm run build` inside `site/` when touching front-end code.ã€F:Makefileâ€ L1-L80ã€‘
4. Open a pull request summarising changes, impacted artefacts, lint/test status, and provenance for new data or references. Follow the lineage guidance in `docs/CONTRIBUTING_SERIES.md` when citing ACX specifications.ã€F:docs/CONTRIBUTING_SERIES.mdâ€ L1-L120ã€‘
5. Expect review on data hygiene, reproducibility, accessibility, and reference integrity. Updates to `data/` or `calc/references/` must include provenance and audit script updates when relevant.

Coding standards:

- Extend `calc/schema.py` with new Pydantic models instead of bypassing validation.
- Avoid broad exception catches; allow validation to fail fast.
- Keep Plotly figure builders deterministic (sorted keys, explicit colour ordering, shared templates via `calc.ui.theme`).ã€F:calc/ui/theme.pyâ€ L1-L120ã€‘
- Do not commit generated artefacts (`build/`, `dist/`, `calc/outputs/`, `site/dist`).
- Honour the serial/traceability process outlined in `docs/CONTRIBUTING_SERIES.md`.

---

## Security & compliance

- **Data isolation** â€“ Derived outputs live in hashed directories and are cleaned only through guarded paths (`calc.derive.is_safe_output_dir`).ã€F:calc/derive.pyâ€ L292-L336ã€‘
- **Dependency governance** â€“ Poetry and npm lockfiles pin versions; `tools/sbom.py` emits CycloneDX SBOMs and pip-audit is available for vulnerability scanning.ã€F:tools/sbom.pyâ€ L1-L120ã€‘ã€F:site/package-lock.jsonâ€ L1-L40ã€‘
- **Runtime posture** â€“ Dash, the static site, and the Cloudflare Worker all read precomputed bundles; no live data collection occurs at runtime. The Pages Function proxies traffic without persisting secrets and applies immutable caching headers.ã€F:functions/carbon-acx/[[path]].tsâ€ L1-L200ã€‘
- **Access control** â€“ The compute Worker normalises input, rejects invalid types, and delegates to `calc.service.compute_profile`, limiting injection risks.ã€F:workers/compute/index.tsâ€ L1-L86ã€‘
- **Licensing** â€“ Released under the MIT License (`LICENSE`). Attribute new data sources when expanding the dataset.

---

## Roadmap

| Status | Milestone | Description |
| --- | --- | --- |
| âœ… | Immutable data builds | Content-hashed artefact directories with manifest pointers, dependency chains, and reference parity across clients.ã€F:calc/derive.pyâ€ L1474-L1534ã€‘ |
| âœ… | Multi-channel delivery | Dash client, static React/Vite bundle, Pages Function, and Worker API share derived Plotly payloads and disclosure copy.ã€F:app/app.pyâ€ L65-L132ã€‘ã€F:site/src/components/VizCanvas.tsxâ€ L1-L160ã€‘ |
| âœ… | Backend parity | CSV, DuckDB, and SQLite datastores validated through automated parity tests for consistent exports.ã€F:tests/test_backend_parity.pyâ€ L1-L47ã€‘ã€F:tests/test_dal_sql.pyâ€ L1-L120ã€‘ |
| âœ… | Intensity narratives | Functional-unit intensity builder feeds UI narratives and CSV downloads, keeping comparisons aligned across channels.ã€F:calc/derive.pyâ€ L1330-L1496ã€‘ã€F:app/lib/narratives.pyâ€ L1-L160ã€‘ |
| ğŸš§ | Release automation | Fill in the `make release` placeholder with scripted tagging, changelog updates, and Pages deployments. |
| ğŸš§ | Dataset refresh tooling | Extend maintenance calendar automation for moving `_staged/` data into `data/` with provenance metadata.ã€F:docs/MAINTENANCE_CALENDAR.mdâ€ L1-L80ã€‘ |
| ğŸš§ | Worker dataset integration | Replace the demo dataset in `workers/compute/runtime.ts` with generated artefacts or live `calc.service` bindings and document deployment playbooks. |
| ğŸ§­ | Additional datastore backends | Explore new `DataStore` implementations (e.g. Postgres) via the existing abstraction in `calc.dal` and `calc.dal_sql`. |
| ğŸ§­ | Expanded visualisations | Prototype intensity waterfalls, cohort comparisons, or sensitivity analyses using `calc.figures`. |
| ğŸ§­ | Live API surface | Wrap `calc.api` aggregates and `compute_profile` in an authenticated HTTP service once security requirements are defined. |

Legend: âœ… implemented Â· ğŸš§ in-progress or partially scaffolded Â· ğŸ§­ planned/under evaluation.

---

## FAQ & troubleshooting

- **Why does `calc/outputs` stay empty?** Artefacts are written to hashed directories under `dist/artifacts/<hash>`. Check `dist/artifacts/latest-build.json` or use helper scripts to locate the latest build.ã€F:calc/derive.pyâ€ L1474-L1542ã€‘
- **`python -m calc.derive export` refuses to clear my output directory.** The pipeline protects against deleting unintended paths. Point `--output-root` inside `dist/artifacts` or set `ACX_ALLOW_OUTPUT_RM=1` when the target is safe.ã€F:calc/derive.pyâ€ L292-L336ã€‘
- **The Dash app cannot find figures.** Ensure `make build` ran successfully and `ACX_ARTIFACT_DIR` (if set) points to a directory containing `figures/*.json` and `manifest.json`.
- **How do I add a new reference?** Add an IEEE-formatted text file in `calc/references/` and reference its stem in CSV data before rebuilding. The citation resolver keeps numbering consistent across outputs.ã€F:calc/citations.pyâ€ L1-L72ã€‘
- **Plotly figures look different locally vs CI.** Confirm `kaleido` is installed (via `make install`) and avoid locale-dependent formatting. Re-run `make build` to regenerate consistent Plotly JSON.
- **Node build fails with an engine warning.** Vite 5 requires Node.js â‰¥ 18. Upgrade Node locally or use the version pinned in `.nvmrc` if present.
- **Cloudflare Pages serves HTML for JSON routes.** Deploy the Pages Function alongside the static site and keep `_headers`/`_redirects` intact; `make package` prepares them automatically. `CARBON_ACX_ORIGIN` should be an absolute origin without a trailing slash.ã€F:functions/carbon-acx/[[path]].tsâ€ L131-L200ã€‘
- **Worker API returns demo data only.** Replace `workers/compute/runtime.ts` with generated artefacts or adapt the Worker to call a hosted `compute_profile` endpoint for production use.
- **Debugging data loads.** Use `scripts/dev_diag.sh` to compare artefact headers locally and in production. The script respects `PUBLIC_BASE_PATH` so you can validate both local static bundles and Pages domains.ã€F:scripts/dev_diag.shâ€ L1-L16ã€‘

---

## References

- `CONTRIBUTING.md`
- `docs/CHANGELOG.md`
- `docs/MAINTENANCE_CALENDAR.md`
- `docs/ONLINE_METHOD_NOTES.md`
- `docs/WHAT_RUNS_WHERE.md`
- `docs/TESTING_NOTES.md`
- `docs/deploy.md`
- `docs/routes.md`
- `LICENSE`

---

_Note: To satisfy repo hygiene tests, avoid using the contiguous token spelled â€œF a s t A P Iâ€ in docs._

---

## Serials & traceability

Refer to `docs/CONTRIBUTING_SERIES.md` for guidance on citing ACX specifications, CDX prompts, and the PR lineage required for every contribution.ã€F:docs/CONTRIBUTING_SERIES.mdâ€ L1-L120ã€‘

## Architecture extensions

### ACX041 View Provenance Module

The ACX041 extension adds signed hash-chains for every published figure. The derivation pipeline now emits `calc/outputs/manifests/*.json` files that stitch together dataset digests, figure payload hashes, reference checksums, and promptware lineage so any client can verify tampering before presenting data. The manifests ride alongside the immutable artefact bundle and are packaged with the static site for Cloudflare Pages deployments.ã€F:calc/manifest.pyâ€ L1-L209ã€‘ã€F:calc/derive.pyâ€ L1506-L1515ã€‘ã€F:scripts/prepare_pages_bundle.pyâ€ L1-L82ã€‘

Client tooling consumes the same manifest format to disable downloads or watermark plots when verification fails; see `docs/ACX041_View_Provenance_Module_v1_2.md` for the integration checklist.ã€F:docs/ACX041_View_Provenance_Module_v1_2.mdâ€ L1-L120ã€‘
