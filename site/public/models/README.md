# Local model assets

The `/public/models` directory stores WebLLM model artifacts that are served
locally by the site. Each model is loaded lazily by the browser when a visitor
opens `/chat` and no network requests are made to external providers.

## Directory layout

Place each model inside a folder that matches the model identifier used by the
app. For example, with the default configuration the application looks for:

```
public/models/qwen2.5-1.5b-instruct-q4f16_1/
```

The folder must contain the files generated by `mlc-llm` for WebGPU deployment:

- `mlc-chat-config.json`
- weight shards (e.g. `params_shard_*.safetensors`)
- tokenizer files (JSON / vocab resources)
- cache descriptors (e.g. `ndarray-cache.json`, `ndarray-cache.bin`)
- the WebGPU runtime (`*.wasm`)

WebLLM expects the WebGPU runtime filename to match the prebuilt artifact. For
`qwen2.5-1.5b-instruct-q4f16_1` the runtime shipped by MLC is named:

```
Qwen2-1.5B-Instruct-q4f16_1-ctx4k_cs1k-webgpu.wasm
```

Make sure this file (and any accompanying JSON) is placed inside the model
folder. The adapter rewrites the download URL so that the runtime is served from
`/models/<MODEL_ID>/<FILENAME>.wasm`.

## Obtaining weights

Run the helper script to download the official package from Hugging Face:

```bash
pnpm download-model
```

Set `HF_TOKEN` (or `HUGGING_FACE_HUB_TOKEN`) if your environment requires an
authenticated request. To skip the automatic download during `pnpm dev`/`pnpm
build`, export `SKIP_WEBLLM_DOWNLOAD=true`.

If you prefer a manual workflow, clone the repository with `git lfs` and copy
the files into the model directory:

```bash
git lfs install
git clone https://huggingface.co/mlc-ai/Qwen2.5-1.5B-Instruct-q4f16_1-MLC
```

Copy the contents of the cloned repository into the folder mentioned above.
Only the files are requiredâ€”the `.git` directory is not needed for deployment.

## Configuring the site

Set the model identifier via the environment:

```
VITE_MODEL_ID=qwen2.5-1.5b-instruct-q4f16_1
```

You can override the model at runtime by appending `?model=<MODEL_ID>` to the
chat URL. Whatever value is supplied must correspond to a directory within
`public/models`.
